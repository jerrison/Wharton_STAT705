{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STAT 705 FALL 2018 ASSIGNMENT 2\n",
    "#### NAME: Jerrison Li\n",
    "\n",
    "#### If in a question I refer to a function that we have not seen in class, \n",
    "#### then use the help facility to find out about it.\n",
    "#### Insert your answers under each question.\n",
    "#### Submit your solutions to Canvas as a plain text file.\n",
    "\n",
    "#### Download the .Rdata file called \"hw2_2018.Rdata\" available on Canvas and read it in to R using R-studio.\n",
    "#### This will bring all the objects you need for the homework straight into R-studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(file = \"hw2_2018.rdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "29585754"
      ],
      "text/latex": [
       "29585754"
      ],
      "text/markdown": [
       "29585754"
      ],
      "text/plain": [
       "[1] 29585754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "29585754"
      ],
      "text/latex": [
       "29585754"
      ],
      "text/markdown": [
       "29585754"
      ],
      "text/plain": [
       "[1] 29585754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$FULLNAME\n",
      "[1] \"Chandigarh\"\n",
      "\n",
      "$COUNTRY\n",
      "[1] \"India\"\n",
      "\n",
      "$ANNUALSTATS\n",
      "$ANNUALSTATS$NUMPASSENGERS\n",
      "[1] 2137739\n",
      "\n",
      "$ANNUALSTATS$NUMFLIGHTS\n",
      "[1] 18715\n",
      "\n",
      "\n",
      "$FULLNAME\n",
      "[1] \"Tocumen International Airport\"\n",
      "\n",
      "$COUNTRY\n",
      "[1] \"Panama\"\n",
      "\n",
      "$ANNUALSTATS\n",
      "$ANNUALSTATS$NUMPASSENGERS\n",
      "[1] 15616065\n",
      "\n",
      "$ANNUALSTATS$NUMFLIGHTS\n",
      "[1] 10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Q1. (6pts. )You will have imported a list called \"mystery.list\".\n",
    "#### Inspect the list and describe briefly in words what is in it.\n",
    "\n",
    "#a. Description: \n",
    "# Answer: It seems like list of length 3, each of them consisting of \n",
    "# information about airports. The three airports are LHR: London Heathrow, PHL:\n",
    "# Philadelphia, and IXC: Chandigarh. The information includes, full name, \n",
    "# country, annual number of passengers and annual number of flights.\n",
    "\n",
    "#b. Use two different R syntaxes for extracting *just* the number of passengers \n",
    "# who fly through PHL (not the sub-list containing the number of passengers).\n",
    "\n",
    "#b(i)\n",
    "# Answer:\n",
    "# Using $ sign: mystery.list$PHL$ANNUALSTATS$NUMPASSENGERS\n",
    "# Output: 29585754\n",
    "mystery.list$PHL$ANNUALSTATS$NUMPASSENGERS\n",
    "\n",
    "#b(ii)\n",
    "# Answer:\n",
    "# Using brackets: mystery.list[[2]][[3]][[1]]\n",
    "# Output: 29585754\n",
    "mystery.list[[2]][[3]][[1]]\n",
    "\n",
    "#c The country for Chandigarh is incorrect. It is in India. Write code to \n",
    "# correct this mistake in the list and then print the corrected list.\n",
    "# Answer:\n",
    "mystery.list$IXC$COUNTRY <- \"India\"\n",
    "print(mystery.list$IXC)\n",
    "# Output:\n",
    "# $FULLNAME\n",
    "# [1] \"Chandigarh\"\n",
    "\n",
    "# $COUNTRY\n",
    "# [1] \"India\"\n",
    "\n",
    "# $ANNUALSTATS\n",
    "# $ANNUALSTATS$NUMPASSENGERS\n",
    "# [1] 2137739\n",
    "\n",
    "# $ANNUALSTATS$NUMFLIGHTS\n",
    "# [1] 18715\n",
    "\n",
    "#d Write code to add the airport nearest your hometown to the list along with \n",
    "# its relevant attributes, and print just this new \n",
    "#  element of the updated list (if you don't know the attributes, guess them!)\n",
    "# Answer:\n",
    "PTY <- list(\"FULLNAME\"= \"Tocumen International Airport\",\n",
    "           \"COUNTRY\"= \"Panama\",\n",
    "           \"ANNUALSTATS\"= list(\"NUMPASSENGERS\"= 15616065, \n",
    "                            \"NUMFLIGHTS\"= 10000))\n",
    "mystery.list$PTY <- PTY\n",
    "print(mystery.list$PTY)\n",
    "# Output\n",
    "# $FULLNAME\n",
    "# [1] \"Tocumen International Airport\"\n",
    "\n",
    "# $COUNTRY\n",
    "# [1] \"Panama\"\n",
    "\n",
    "# $ANNUALSTATS\n",
    "# $ANNUALSTATS$NUMPASSENGERS\n",
    "# [1] 15616065\n",
    "\n",
    "# $ANNUALSTATS$NUMFLIGHTS\n",
    "# [1] 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>252</li>\n",
       "\t<li>4</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 252\n",
       "\\item 4\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 252\n",
       "2. 4\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 252   4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "09/18/2017\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'01/02/2018'</li>\n",
       "\t\t<li>'01/03/2018'</li>\n",
       "\t\t<li>'01/04/2018'</li>\n",
       "\t\t<li>'01/05/2018'</li>\n",
       "\t\t<li>'01/08/2018'</li>\n",
       "\t\t<li>'01/09/2018'</li>\n",
       "\t\t<li>'01/10/2018'</li>\n",
       "\t\t<li>'01/11/2018'</li>\n",
       "\t\t<li>'01/12/2018'</li>\n",
       "\t\t<li>'01/16/2018'</li>\n",
       "\t\t<li>'01/17/2018'</li>\n",
       "\t\t<li>'01/18/2018'</li>\n",
       "\t\t<li>'01/19/2018'</li>\n",
       "\t\t<li>'01/22/2018'</li>\n",
       "\t\t<li>'01/23/2018'</li>\n",
       "\t\t<li>'01/24/2018'</li>\n",
       "\t\t<li>'01/25/2018'</li>\n",
       "\t\t<li>'01/26/2018'</li>\n",
       "\t\t<li>'01/29/2018'</li>\n",
       "\t\t<li>'01/30/2018'</li>\n",
       "\t\t<li>'01/31/2018'</li>\n",
       "\t\t<li>'02/01/2018'</li>\n",
       "\t\t<li>'02/02/2018'</li>\n",
       "\t\t<li>'02/05/2018'</li>\n",
       "\t\t<li>'02/06/2018'</li>\n",
       "\t\t<li>'02/07/2018'</li>\n",
       "\t\t<li>'02/08/2018'</li>\n",
       "\t\t<li>'02/09/2018'</li>\n",
       "\t\t<li>'02/12/2018'</li>\n",
       "\t\t<li>'02/13/2018'</li>\n",
       "\t\t<li>'02/14/2018'</li>\n",
       "\t\t<li>'02/15/2018'</li>\n",
       "\t\t<li>'02/16/2018'</li>\n",
       "\t\t<li>'02/20/2018'</li>\n",
       "\t\t<li>'02/21/2018'</li>\n",
       "\t\t<li>'02/22/2018'</li>\n",
       "\t\t<li>'02/23/2018'</li>\n",
       "\t\t<li>'02/26/2018'</li>\n",
       "\t\t<li>'02/27/2018'</li>\n",
       "\t\t<li>'02/28/2018'</li>\n",
       "\t\t<li>'03/01/2018'</li>\n",
       "\t\t<li>'03/02/2018'</li>\n",
       "\t\t<li>'03/05/2018'</li>\n",
       "\t\t<li>'03/06/2018'</li>\n",
       "\t\t<li>'03/07/2018'</li>\n",
       "\t\t<li>'03/08/2018'</li>\n",
       "\t\t<li>'03/09/2018'</li>\n",
       "\t\t<li>'03/12/2018'</li>\n",
       "\t\t<li>'03/13/2018'</li>\n",
       "\t\t<li>'03/14/2018'</li>\n",
       "\t\t<li>'03/15/2018'</li>\n",
       "\t\t<li>'03/16/2018'</li>\n",
       "\t\t<li>'03/19/2018'</li>\n",
       "\t\t<li>'03/20/2018'</li>\n",
       "\t\t<li>'03/21/2018'</li>\n",
       "\t\t<li>'03/22/2018'</li>\n",
       "\t\t<li>'03/23/2018'</li>\n",
       "\t\t<li>'03/26/2018'</li>\n",
       "\t\t<li>'03/27/2018'</li>\n",
       "\t\t<li>'03/28/2018'</li>\n",
       "\t\t<li>'03/29/2018'</li>\n",
       "\t\t<li>'04/02/2018'</li>\n",
       "\t\t<li>'04/03/2018'</li>\n",
       "\t\t<li>'04/04/2018'</li>\n",
       "\t\t<li>'04/05/2018'</li>\n",
       "\t\t<li>'04/06/2018'</li>\n",
       "\t\t<li>'04/09/2018'</li>\n",
       "\t\t<li>'04/10/2018'</li>\n",
       "\t\t<li>'04/11/2018'</li>\n",
       "\t\t<li>'04/12/2018'</li>\n",
       "\t\t<li>'04/13/2018'</li>\n",
       "\t\t<li>'04/16/2018'</li>\n",
       "\t\t<li>'04/17/2018'</li>\n",
       "\t\t<li>'04/18/2018'</li>\n",
       "\t\t<li>'04/19/2018'</li>\n",
       "\t\t<li>'04/20/2018'</li>\n",
       "\t\t<li>'04/23/2018'</li>\n",
       "\t\t<li>'04/24/2018'</li>\n",
       "\t\t<li>'04/25/2018'</li>\n",
       "\t\t<li>'04/26/2018'</li>\n",
       "\t\t<li>'04/27/2018'</li>\n",
       "\t\t<li>'04/30/2018'</li>\n",
       "\t\t<li>'05/01/2018'</li>\n",
       "\t\t<li>'05/02/2018'</li>\n",
       "\t\t<li>'05/03/2018'</li>\n",
       "\t\t<li>'05/04/2018'</li>\n",
       "\t\t<li>'05/07/2018'</li>\n",
       "\t\t<li>'05/08/2018'</li>\n",
       "\t\t<li>'05/09/2018'</li>\n",
       "\t\t<li>'05/10/2018'</li>\n",
       "\t\t<li>'05/11/2018'</li>\n",
       "\t\t<li>'05/14/2018'</li>\n",
       "\t\t<li>'05/15/2018'</li>\n",
       "\t\t<li>'05/16/2018'</li>\n",
       "\t\t<li>'05/17/2018'</li>\n",
       "\t\t<li>'05/18/2018'</li>\n",
       "\t\t<li>'05/21/2018'</li>\n",
       "\t\t<li>'05/22/2018'</li>\n",
       "\t\t<li>'05/23/2018'</li>\n",
       "\t\t<li>'05/24/2018'</li>\n",
       "\t\t<li>'05/25/2018'</li>\n",
       "\t\t<li>'05/29/2018'</li>\n",
       "\t\t<li>'05/30/2018'</li>\n",
       "\t\t<li>'05/31/2018'</li>\n",
       "\t\t<li>'06/01/2018'</li>\n",
       "\t\t<li>'06/04/2018'</li>\n",
       "\t\t<li>'06/05/2018'</li>\n",
       "\t\t<li>'06/06/2018'</li>\n",
       "\t\t<li>'06/07/2018'</li>\n",
       "\t\t<li>'06/08/2018'</li>\n",
       "\t\t<li>'06/11/2018'</li>\n",
       "\t\t<li>'06/12/2018'</li>\n",
       "\t\t<li>'06/13/2018'</li>\n",
       "\t\t<li>'06/14/2018'</li>\n",
       "\t\t<li>'06/15/2018'</li>\n",
       "\t\t<li>'06/18/2018'</li>\n",
       "\t\t<li>'06/19/2018'</li>\n",
       "\t\t<li>'06/20/2018'</li>\n",
       "\t\t<li>'06/21/2018'</li>\n",
       "\t\t<li>'06/22/2018'</li>\n",
       "\t\t<li>'06/25/2018'</li>\n",
       "\t\t<li>'06/26/2018'</li>\n",
       "\t\t<li>'06/27/2018'</li>\n",
       "\t\t<li>'06/28/2018'</li>\n",
       "\t\t<li>'06/29/2018'</li>\n",
       "\t\t<li>'07/02/2018'</li>\n",
       "\t\t<li>'07/03/2018'</li>\n",
       "\t\t<li>'07/05/2018'</li>\n",
       "\t\t<li>'07/06/2018'</li>\n",
       "\t\t<li>'07/09/2018'</li>\n",
       "\t\t<li>'07/10/2018'</li>\n",
       "\t\t<li>'07/11/2018'</li>\n",
       "\t\t<li>'07/12/2018'</li>\n",
       "\t\t<li>'07/13/2018'</li>\n",
       "\t\t<li>'07/16/2018'</li>\n",
       "\t\t<li>'07/17/2018'</li>\n",
       "\t\t<li>'07/18/2018'</li>\n",
       "\t\t<li>'07/19/2018'</li>\n",
       "\t\t<li>'07/20/2018'</li>\n",
       "\t\t<li>'07/23/2018'</li>\n",
       "\t\t<li>'07/24/2018'</li>\n",
       "\t\t<li>'07/25/2018'</li>\n",
       "\t\t<li>'07/26/2018'</li>\n",
       "\t\t<li>'07/27/2018'</li>\n",
       "\t\t<li>'07/30/2018'</li>\n",
       "\t\t<li>'07/31/2018'</li>\n",
       "\t\t<li>'08/01/2018'</li>\n",
       "\t\t<li>'08/02/2018'</li>\n",
       "\t\t<li>'08/03/2018'</li>\n",
       "\t\t<li>'08/06/2018'</li>\n",
       "\t\t<li>'08/07/2018'</li>\n",
       "\t\t<li>'08/08/2018'</li>\n",
       "\t\t<li>'08/09/2018'</li>\n",
       "\t\t<li>'08/10/2018'</li>\n",
       "\t\t<li>'08/13/2018'</li>\n",
       "\t\t<li>'08/14/2018'</li>\n",
       "\t\t<li>'08/15/2018'</li>\n",
       "\t\t<li>'08/16/2018'</li>\n",
       "\t\t<li>'08/17/2018'</li>\n",
       "\t\t<li>'08/20/2018'</li>\n",
       "\t\t<li>'08/21/2018'</li>\n",
       "\t\t<li>'08/22/2018'</li>\n",
       "\t\t<li>'08/23/2018'</li>\n",
       "\t\t<li>'08/24/2018'</li>\n",
       "\t\t<li>'08/27/2018'</li>\n",
       "\t\t<li>'08/28/2018'</li>\n",
       "\t\t<li>'08/29/2018'</li>\n",
       "\t\t<li>'08/30/2018'</li>\n",
       "\t\t<li>'08/31/2018'</li>\n",
       "\t\t<li>'09/01/2017'</li>\n",
       "\t\t<li>'09/05/2017'</li>\n",
       "\t\t<li>'09/06/2017'</li>\n",
       "\t\t<li>'09/07/2017'</li>\n",
       "\t\t<li>'09/08/2017'</li>\n",
       "\t\t<li>'09/11/2017'</li>\n",
       "\t\t<li>'09/12/2017'</li>\n",
       "\t\t<li>'09/13/2017'</li>\n",
       "\t\t<li>'09/14/2017'</li>\n",
       "\t\t<li>'09/15/2017'</li>\n",
       "\t\t<li>'09/18/2017'</li>\n",
       "\t\t<li>'09/19/2017'</li>\n",
       "\t\t<li>'09/20/2017'</li>\n",
       "\t\t<li>'09/21/2017'</li>\n",
       "\t\t<li>'09/22/2017'</li>\n",
       "\t\t<li>'09/25/2017'</li>\n",
       "\t\t<li>'09/26/2017'</li>\n",
       "\t\t<li>'09/27/2017'</li>\n",
       "\t\t<li>'09/28/2017'</li>\n",
       "\t\t<li>'09/29/2017'</li>\n",
       "\t\t<li>'10/02/2017'</li>\n",
       "\t\t<li>'10/03/2017'</li>\n",
       "\t\t<li>'10/04/2017'</li>\n",
       "\t\t<li>'10/05/2017'</li>\n",
       "\t\t<li>'10/06/2017'</li>\n",
       "\t\t<li>'10/09/2017'</li>\n",
       "\t\t<li>'10/10/2017'</li>\n",
       "\t\t<li>'10/11/2017'</li>\n",
       "\t\t<li>'10/12/2017'</li>\n",
       "\t\t<li>'10/13/2017'</li>\n",
       "\t\t<li>'10/16/2017'</li>\n",
       "\t\t<li>'10/17/2017'</li>\n",
       "\t\t<li>'10/18/2017'</li>\n",
       "\t\t<li>'10/19/2017'</li>\n",
       "\t\t<li>'10/20/2017'</li>\n",
       "\t\t<li>'10/23/2017'</li>\n",
       "\t\t<li>'10/24/2017'</li>\n",
       "\t\t<li>'10/25/2017'</li>\n",
       "\t\t<li>'10/26/2017'</li>\n",
       "\t\t<li>'10/27/2017'</li>\n",
       "\t\t<li>'10/30/2017'</li>\n",
       "\t\t<li>'10/31/2017'</li>\n",
       "\t\t<li>'11/01/2017'</li>\n",
       "\t\t<li>'11/02/2017'</li>\n",
       "\t\t<li>'11/03/2017'</li>\n",
       "\t\t<li>'11/06/2017'</li>\n",
       "\t\t<li>'11/07/2017'</li>\n",
       "\t\t<li>'11/08/2017'</li>\n",
       "\t\t<li>'11/09/2017'</li>\n",
       "\t\t<li>'11/10/2017'</li>\n",
       "\t\t<li>'11/13/2017'</li>\n",
       "\t\t<li>'11/14/2017'</li>\n",
       "\t\t<li>'11/15/2017'</li>\n",
       "\t\t<li>'11/16/2017'</li>\n",
       "\t\t<li>'11/17/2017'</li>\n",
       "\t\t<li>'11/20/2017'</li>\n",
       "\t\t<li>'11/21/2017'</li>\n",
       "\t\t<li>'11/22/2017'</li>\n",
       "\t\t<li>'11/24/2017'</li>\n",
       "\t\t<li>'11/27/2017'</li>\n",
       "\t\t<li>'11/28/2017'</li>\n",
       "\t\t<li>'11/29/2017'</li>\n",
       "\t\t<li>'11/30/2017'</li>\n",
       "\t\t<li>'12/01/2017'</li>\n",
       "\t\t<li>'12/04/2017'</li>\n",
       "\t\t<li>'12/05/2017'</li>\n",
       "\t\t<li>'12/06/2017'</li>\n",
       "\t\t<li>'12/07/2017'</li>\n",
       "\t\t<li>'12/08/2017'</li>\n",
       "\t\t<li>'12/11/2017'</li>\n",
       "\t\t<li>'12/12/2017'</li>\n",
       "\t\t<li>'12/13/2017'</li>\n",
       "\t\t<li>'12/14/2017'</li>\n",
       "\t\t<li>'12/15/2017'</li>\n",
       "\t\t<li>'12/18/2017'</li>\n",
       "\t\t<li>'12/19/2017'</li>\n",
       "\t\t<li>'12/20/2017'</li>\n",
       "\t\t<li>'12/21/2017'</li>\n",
       "\t\t<li>'12/22/2017'</li>\n",
       "\t\t<li>'12/26/2017'</li>\n",
       "\t\t<li>'12/27/2017'</li>\n",
       "\t\t<li>'12/28/2017'</li>\n",
       "\t\t<li>'12/29/2017'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "09/18/2017\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item '01/02/2018'\n",
       "\\item '01/03/2018'\n",
       "\\item '01/04/2018'\n",
       "\\item '01/05/2018'\n",
       "\\item '01/08/2018'\n",
       "\\item '01/09/2018'\n",
       "\\item '01/10/2018'\n",
       "\\item '01/11/2018'\n",
       "\\item '01/12/2018'\n",
       "\\item '01/16/2018'\n",
       "\\item '01/17/2018'\n",
       "\\item '01/18/2018'\n",
       "\\item '01/19/2018'\n",
       "\\item '01/22/2018'\n",
       "\\item '01/23/2018'\n",
       "\\item '01/24/2018'\n",
       "\\item '01/25/2018'\n",
       "\\item '01/26/2018'\n",
       "\\item '01/29/2018'\n",
       "\\item '01/30/2018'\n",
       "\\item '01/31/2018'\n",
       "\\item '02/01/2018'\n",
       "\\item '02/02/2018'\n",
       "\\item '02/05/2018'\n",
       "\\item '02/06/2018'\n",
       "\\item '02/07/2018'\n",
       "\\item '02/08/2018'\n",
       "\\item '02/09/2018'\n",
       "\\item '02/12/2018'\n",
       "\\item '02/13/2018'\n",
       "\\item '02/14/2018'\n",
       "\\item '02/15/2018'\n",
       "\\item '02/16/2018'\n",
       "\\item '02/20/2018'\n",
       "\\item '02/21/2018'\n",
       "\\item '02/22/2018'\n",
       "\\item '02/23/2018'\n",
       "\\item '02/26/2018'\n",
       "\\item '02/27/2018'\n",
       "\\item '02/28/2018'\n",
       "\\item '03/01/2018'\n",
       "\\item '03/02/2018'\n",
       "\\item '03/05/2018'\n",
       "\\item '03/06/2018'\n",
       "\\item '03/07/2018'\n",
       "\\item '03/08/2018'\n",
       "\\item '03/09/2018'\n",
       "\\item '03/12/2018'\n",
       "\\item '03/13/2018'\n",
       "\\item '03/14/2018'\n",
       "\\item '03/15/2018'\n",
       "\\item '03/16/2018'\n",
       "\\item '03/19/2018'\n",
       "\\item '03/20/2018'\n",
       "\\item '03/21/2018'\n",
       "\\item '03/22/2018'\n",
       "\\item '03/23/2018'\n",
       "\\item '03/26/2018'\n",
       "\\item '03/27/2018'\n",
       "\\item '03/28/2018'\n",
       "\\item '03/29/2018'\n",
       "\\item '04/02/2018'\n",
       "\\item '04/03/2018'\n",
       "\\item '04/04/2018'\n",
       "\\item '04/05/2018'\n",
       "\\item '04/06/2018'\n",
       "\\item '04/09/2018'\n",
       "\\item '04/10/2018'\n",
       "\\item '04/11/2018'\n",
       "\\item '04/12/2018'\n",
       "\\item '04/13/2018'\n",
       "\\item '04/16/2018'\n",
       "\\item '04/17/2018'\n",
       "\\item '04/18/2018'\n",
       "\\item '04/19/2018'\n",
       "\\item '04/20/2018'\n",
       "\\item '04/23/2018'\n",
       "\\item '04/24/2018'\n",
       "\\item '04/25/2018'\n",
       "\\item '04/26/2018'\n",
       "\\item '04/27/2018'\n",
       "\\item '04/30/2018'\n",
       "\\item '05/01/2018'\n",
       "\\item '05/02/2018'\n",
       "\\item '05/03/2018'\n",
       "\\item '05/04/2018'\n",
       "\\item '05/07/2018'\n",
       "\\item '05/08/2018'\n",
       "\\item '05/09/2018'\n",
       "\\item '05/10/2018'\n",
       "\\item '05/11/2018'\n",
       "\\item '05/14/2018'\n",
       "\\item '05/15/2018'\n",
       "\\item '05/16/2018'\n",
       "\\item '05/17/2018'\n",
       "\\item '05/18/2018'\n",
       "\\item '05/21/2018'\n",
       "\\item '05/22/2018'\n",
       "\\item '05/23/2018'\n",
       "\\item '05/24/2018'\n",
       "\\item '05/25/2018'\n",
       "\\item '05/29/2018'\n",
       "\\item '05/30/2018'\n",
       "\\item '05/31/2018'\n",
       "\\item '06/01/2018'\n",
       "\\item '06/04/2018'\n",
       "\\item '06/05/2018'\n",
       "\\item '06/06/2018'\n",
       "\\item '06/07/2018'\n",
       "\\item '06/08/2018'\n",
       "\\item '06/11/2018'\n",
       "\\item '06/12/2018'\n",
       "\\item '06/13/2018'\n",
       "\\item '06/14/2018'\n",
       "\\item '06/15/2018'\n",
       "\\item '06/18/2018'\n",
       "\\item '06/19/2018'\n",
       "\\item '06/20/2018'\n",
       "\\item '06/21/2018'\n",
       "\\item '06/22/2018'\n",
       "\\item '06/25/2018'\n",
       "\\item '06/26/2018'\n",
       "\\item '06/27/2018'\n",
       "\\item '06/28/2018'\n",
       "\\item '06/29/2018'\n",
       "\\item '07/02/2018'\n",
       "\\item '07/03/2018'\n",
       "\\item '07/05/2018'\n",
       "\\item '07/06/2018'\n",
       "\\item '07/09/2018'\n",
       "\\item '07/10/2018'\n",
       "\\item '07/11/2018'\n",
       "\\item '07/12/2018'\n",
       "\\item '07/13/2018'\n",
       "\\item '07/16/2018'\n",
       "\\item '07/17/2018'\n",
       "\\item '07/18/2018'\n",
       "\\item '07/19/2018'\n",
       "\\item '07/20/2018'\n",
       "\\item '07/23/2018'\n",
       "\\item '07/24/2018'\n",
       "\\item '07/25/2018'\n",
       "\\item '07/26/2018'\n",
       "\\item '07/27/2018'\n",
       "\\item '07/30/2018'\n",
       "\\item '07/31/2018'\n",
       "\\item '08/01/2018'\n",
       "\\item '08/02/2018'\n",
       "\\item '08/03/2018'\n",
       "\\item '08/06/2018'\n",
       "\\item '08/07/2018'\n",
       "\\item '08/08/2018'\n",
       "\\item '08/09/2018'\n",
       "\\item '08/10/2018'\n",
       "\\item '08/13/2018'\n",
       "\\item '08/14/2018'\n",
       "\\item '08/15/2018'\n",
       "\\item '08/16/2018'\n",
       "\\item '08/17/2018'\n",
       "\\item '08/20/2018'\n",
       "\\item '08/21/2018'\n",
       "\\item '08/22/2018'\n",
       "\\item '08/23/2018'\n",
       "\\item '08/24/2018'\n",
       "\\item '08/27/2018'\n",
       "\\item '08/28/2018'\n",
       "\\item '08/29/2018'\n",
       "\\item '08/30/2018'\n",
       "\\item '08/31/2018'\n",
       "\\item '09/01/2017'\n",
       "\\item '09/05/2017'\n",
       "\\item '09/06/2017'\n",
       "\\item '09/07/2017'\n",
       "\\item '09/08/2017'\n",
       "\\item '09/11/2017'\n",
       "\\item '09/12/2017'\n",
       "\\item '09/13/2017'\n",
       "\\item '09/14/2017'\n",
       "\\item '09/15/2017'\n",
       "\\item '09/18/2017'\n",
       "\\item '09/19/2017'\n",
       "\\item '09/20/2017'\n",
       "\\item '09/21/2017'\n",
       "\\item '09/22/2017'\n",
       "\\item '09/25/2017'\n",
       "\\item '09/26/2017'\n",
       "\\item '09/27/2017'\n",
       "\\item '09/28/2017'\n",
       "\\item '09/29/2017'\n",
       "\\item '10/02/2017'\n",
       "\\item '10/03/2017'\n",
       "\\item '10/04/2017'\n",
       "\\item '10/05/2017'\n",
       "\\item '10/06/2017'\n",
       "\\item '10/09/2017'\n",
       "\\item '10/10/2017'\n",
       "\\item '10/11/2017'\n",
       "\\item '10/12/2017'\n",
       "\\item '10/13/2017'\n",
       "\\item '10/16/2017'\n",
       "\\item '10/17/2017'\n",
       "\\item '10/18/2017'\n",
       "\\item '10/19/2017'\n",
       "\\item '10/20/2017'\n",
       "\\item '10/23/2017'\n",
       "\\item '10/24/2017'\n",
       "\\item '10/25/2017'\n",
       "\\item '10/26/2017'\n",
       "\\item '10/27/2017'\n",
       "\\item '10/30/2017'\n",
       "\\item '10/31/2017'\n",
       "\\item '11/01/2017'\n",
       "\\item '11/02/2017'\n",
       "\\item '11/03/2017'\n",
       "\\item '11/06/2017'\n",
       "\\item '11/07/2017'\n",
       "\\item '11/08/2017'\n",
       "\\item '11/09/2017'\n",
       "\\item '11/10/2017'\n",
       "\\item '11/13/2017'\n",
       "\\item '11/14/2017'\n",
       "\\item '11/15/2017'\n",
       "\\item '11/16/2017'\n",
       "\\item '11/17/2017'\n",
       "\\item '11/20/2017'\n",
       "\\item '11/21/2017'\n",
       "\\item '11/22/2017'\n",
       "\\item '11/24/2017'\n",
       "\\item '11/27/2017'\n",
       "\\item '11/28/2017'\n",
       "\\item '11/29/2017'\n",
       "\\item '11/30/2017'\n",
       "\\item '12/01/2017'\n",
       "\\item '12/04/2017'\n",
       "\\item '12/05/2017'\n",
       "\\item '12/06/2017'\n",
       "\\item '12/07/2017'\n",
       "\\item '12/08/2017'\n",
       "\\item '12/11/2017'\n",
       "\\item '12/12/2017'\n",
       "\\item '12/13/2017'\n",
       "\\item '12/14/2017'\n",
       "\\item '12/15/2017'\n",
       "\\item '12/18/2017'\n",
       "\\item '12/19/2017'\n",
       "\\item '12/20/2017'\n",
       "\\item '12/21/2017'\n",
       "\\item '12/22/2017'\n",
       "\\item '12/26/2017'\n",
       "\\item '12/27/2017'\n",
       "\\item '12/28/2017'\n",
       "\\item '12/29/2017'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "09/18/2017\n",
       "**Levels**: 1. '01/02/2018'\n",
       "2. '01/03/2018'\n",
       "3. '01/04/2018'\n",
       "4. '01/05/2018'\n",
       "5. '01/08/2018'\n",
       "6. '01/09/2018'\n",
       "7. '01/10/2018'\n",
       "8. '01/11/2018'\n",
       "9. '01/12/2018'\n",
       "10. '01/16/2018'\n",
       "11. '01/17/2018'\n",
       "12. '01/18/2018'\n",
       "13. '01/19/2018'\n",
       "14. '01/22/2018'\n",
       "15. '01/23/2018'\n",
       "16. '01/24/2018'\n",
       "17. '01/25/2018'\n",
       "18. '01/26/2018'\n",
       "19. '01/29/2018'\n",
       "20. '01/30/2018'\n",
       "21. '01/31/2018'\n",
       "22. '02/01/2018'\n",
       "23. '02/02/2018'\n",
       "24. '02/05/2018'\n",
       "25. '02/06/2018'\n",
       "26. '02/07/2018'\n",
       "27. '02/08/2018'\n",
       "28. '02/09/2018'\n",
       "29. '02/12/2018'\n",
       "30. '02/13/2018'\n",
       "31. '02/14/2018'\n",
       "32. '02/15/2018'\n",
       "33. '02/16/2018'\n",
       "34. '02/20/2018'\n",
       "35. '02/21/2018'\n",
       "36. '02/22/2018'\n",
       "37. '02/23/2018'\n",
       "38. '02/26/2018'\n",
       "39. '02/27/2018'\n",
       "40. '02/28/2018'\n",
       "41. '03/01/2018'\n",
       "42. '03/02/2018'\n",
       "43. '03/05/2018'\n",
       "44. '03/06/2018'\n",
       "45. '03/07/2018'\n",
       "46. '03/08/2018'\n",
       "47. '03/09/2018'\n",
       "48. '03/12/2018'\n",
       "49. '03/13/2018'\n",
       "50. '03/14/2018'\n",
       "51. '03/15/2018'\n",
       "52. '03/16/2018'\n",
       "53. '03/19/2018'\n",
       "54. '03/20/2018'\n",
       "55. '03/21/2018'\n",
       "56. '03/22/2018'\n",
       "57. '03/23/2018'\n",
       "58. '03/26/2018'\n",
       "59. '03/27/2018'\n",
       "60. '03/28/2018'\n",
       "61. '03/29/2018'\n",
       "62. '04/02/2018'\n",
       "63. '04/03/2018'\n",
       "64. '04/04/2018'\n",
       "65. '04/05/2018'\n",
       "66. '04/06/2018'\n",
       "67. '04/09/2018'\n",
       "68. '04/10/2018'\n",
       "69. '04/11/2018'\n",
       "70. '04/12/2018'\n",
       "71. '04/13/2018'\n",
       "72. '04/16/2018'\n",
       "73. '04/17/2018'\n",
       "74. '04/18/2018'\n",
       "75. '04/19/2018'\n",
       "76. '04/20/2018'\n",
       "77. '04/23/2018'\n",
       "78. '04/24/2018'\n",
       "79. '04/25/2018'\n",
       "80. '04/26/2018'\n",
       "81. '04/27/2018'\n",
       "82. '04/30/2018'\n",
       "83. '05/01/2018'\n",
       "84. '05/02/2018'\n",
       "85. '05/03/2018'\n",
       "86. '05/04/2018'\n",
       "87. '05/07/2018'\n",
       "88. '05/08/2018'\n",
       "89. '05/09/2018'\n",
       "90. '05/10/2018'\n",
       "91. '05/11/2018'\n",
       "92. '05/14/2018'\n",
       "93. '05/15/2018'\n",
       "94. '05/16/2018'\n",
       "95. '05/17/2018'\n",
       "96. '05/18/2018'\n",
       "97. '05/21/2018'\n",
       "98. '05/22/2018'\n",
       "99. '05/23/2018'\n",
       "100. '05/24/2018'\n",
       "101. '05/25/2018'\n",
       "102. '05/29/2018'\n",
       "103. '05/30/2018'\n",
       "104. '05/31/2018'\n",
       "105. '06/01/2018'\n",
       "106. '06/04/2018'\n",
       "107. '06/05/2018'\n",
       "108. '06/06/2018'\n",
       "109. '06/07/2018'\n",
       "110. '06/08/2018'\n",
       "111. '06/11/2018'\n",
       "112. '06/12/2018'\n",
       "113. '06/13/2018'\n",
       "114. '06/14/2018'\n",
       "115. '06/15/2018'\n",
       "116. '06/18/2018'\n",
       "117. '06/19/2018'\n",
       "118. '06/20/2018'\n",
       "119. '06/21/2018'\n",
       "120. '06/22/2018'\n",
       "121. '06/25/2018'\n",
       "122. '06/26/2018'\n",
       "123. '06/27/2018'\n",
       "124. '06/28/2018'\n",
       "125. '06/29/2018'\n",
       "126. '07/02/2018'\n",
       "127. '07/03/2018'\n",
       "128. '07/05/2018'\n",
       "129. '07/06/2018'\n",
       "130. '07/09/2018'\n",
       "131. '07/10/2018'\n",
       "132. '07/11/2018'\n",
       "133. '07/12/2018'\n",
       "134. '07/13/2018'\n",
       "135. '07/16/2018'\n",
       "136. '07/17/2018'\n",
       "137. '07/18/2018'\n",
       "138. '07/19/2018'\n",
       "139. '07/20/2018'\n",
       "140. '07/23/2018'\n",
       "141. '07/24/2018'\n",
       "142. '07/25/2018'\n",
       "143. '07/26/2018'\n",
       "144. '07/27/2018'\n",
       "145. '07/30/2018'\n",
       "146. '07/31/2018'\n",
       "147. '08/01/2018'\n",
       "148. '08/02/2018'\n",
       "149. '08/03/2018'\n",
       "150. '08/06/2018'\n",
       "151. '08/07/2018'\n",
       "152. '08/08/2018'\n",
       "153. '08/09/2018'\n",
       "154. '08/10/2018'\n",
       "155. '08/13/2018'\n",
       "156. '08/14/2018'\n",
       "157. '08/15/2018'\n",
       "158. '08/16/2018'\n",
       "159. '08/17/2018'\n",
       "160. '08/20/2018'\n",
       "161. '08/21/2018'\n",
       "162. '08/22/2018'\n",
       "163. '08/23/2018'\n",
       "164. '08/24/2018'\n",
       "165. '08/27/2018'\n",
       "166. '08/28/2018'\n",
       "167. '08/29/2018'\n",
       "168. '08/30/2018'\n",
       "169. '08/31/2018'\n",
       "170. '09/01/2017'\n",
       "171. '09/05/2017'\n",
       "172. '09/06/2017'\n",
       "173. '09/07/2017'\n",
       "174. '09/08/2017'\n",
       "175. '09/11/2017'\n",
       "176. '09/12/2017'\n",
       "177. '09/13/2017'\n",
       "178. '09/14/2017'\n",
       "179. '09/15/2017'\n",
       "180. '09/18/2017'\n",
       "181. '09/19/2017'\n",
       "182. '09/20/2017'\n",
       "183. '09/21/2017'\n",
       "184. '09/22/2017'\n",
       "185. '09/25/2017'\n",
       "186. '09/26/2017'\n",
       "187. '09/27/2017'\n",
       "188. '09/28/2017'\n",
       "189. '09/29/2017'\n",
       "190. '10/02/2017'\n",
       "191. '10/03/2017'\n",
       "192. '10/04/2017'\n",
       "193. '10/05/2017'\n",
       "194. '10/06/2017'\n",
       "195. '10/09/2017'\n",
       "196. '10/10/2017'\n",
       "197. '10/11/2017'\n",
       "198. '10/12/2017'\n",
       "199. '10/13/2017'\n",
       "200. '10/16/2017'\n",
       "201. '10/17/2017'\n",
       "202. '10/18/2017'\n",
       "203. '10/19/2017'\n",
       "204. '10/20/2017'\n",
       "205. '10/23/2017'\n",
       "206. '10/24/2017'\n",
       "207. '10/25/2017'\n",
       "208. '10/26/2017'\n",
       "209. '10/27/2017'\n",
       "210. '10/30/2017'\n",
       "211. '10/31/2017'\n",
       "212. '11/01/2017'\n",
       "213. '11/02/2017'\n",
       "214. '11/03/2017'\n",
       "215. '11/06/2017'\n",
       "216. '11/07/2017'\n",
       "217. '11/08/2017'\n",
       "218. '11/09/2017'\n",
       "219. '11/10/2017'\n",
       "220. '11/13/2017'\n",
       "221. '11/14/2017'\n",
       "222. '11/15/2017'\n",
       "223. '11/16/2017'\n",
       "224. '11/17/2017'\n",
       "225. '11/20/2017'\n",
       "226. '11/21/2017'\n",
       "227. '11/22/2017'\n",
       "228. '11/24/2017'\n",
       "229. '11/27/2017'\n",
       "230. '11/28/2017'\n",
       "231. '11/29/2017'\n",
       "232. '11/30/2017'\n",
       "233. '12/01/2017'\n",
       "234. '12/04/2017'\n",
       "235. '12/05/2017'\n",
       "236. '12/06/2017'\n",
       "237. '12/07/2017'\n",
       "238. '12/08/2017'\n",
       "239. '12/11/2017'\n",
       "240. '12/12/2017'\n",
       "241. '12/13/2017'\n",
       "242. '12/14/2017'\n",
       "243. '12/15/2017'\n",
       "244. '12/18/2017'\n",
       "245. '12/19/2017'\n",
       "246. '12/20/2017'\n",
       "247. '12/21/2017'\n",
       "248. '12/22/2017'\n",
       "249. '12/26/2017'\n",
       "250. '12/27/2017'\n",
       "251. '12/28/2017'\n",
       "252. '12/29/2017'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 09/18/2017\n",
       "252 Levels: 01/02/2018 01/03/2018 01/04/2018 01/05/2018 ... 12/29/2017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Date</th><th scope=col>ClosePrice</th><th scope=col>Volume</th><th scope=col>DayOfWeek</th><th scope=col>Return</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>09/01/2017 </td><td>355.40     </td><td>3049500    </td><td>6          </td><td>         NA</td></tr>\n",
       "\t<tr><td>09/05/2017 </td><td>349.59     </td><td>3835100    </td><td>3          </td><td>-0.01634777</td></tr>\n",
       "\t<tr><td>09/06/2017 </td><td>344.53     </td><td>4091400    </td><td>4          </td><td>-0.01447409</td></tr>\n",
       "\t<tr><td>09/07/2017 </td><td>350.61     </td><td>4239200    </td><td>5          </td><td> 0.01764719</td></tr>\n",
       "\t<tr><td>09/08/2017 </td><td>343.40     </td><td>3263500    </td><td>6          </td><td>-0.02056413</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " Date & ClosePrice & Volume & DayOfWeek & Return\\\\\n",
       "\\hline\n",
       "\t 09/01/2017  & 355.40      & 3049500     & 6           &          NA\\\\\n",
       "\t 09/05/2017  & 349.59      & 3835100     & 3           & -0.01634777\\\\\n",
       "\t 09/06/2017  & 344.53      & 4091400     & 4           & -0.01447409\\\\\n",
       "\t 09/07/2017  & 350.61      & 4239200     & 5           &  0.01764719\\\\\n",
       "\t 09/08/2017  & 343.40      & 3263500     & 6           & -0.02056413\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Date | ClosePrice | Volume | DayOfWeek | Return | \n",
       "|---|---|---|---|---|\n",
       "| 09/01/2017  | 355.40      | 3049500     | 6           |          NA | \n",
       "| 09/05/2017  | 349.59      | 3835100     | 3           | -0.01634777 | \n",
       "| 09/06/2017  | 344.53      | 4091400     | 4           | -0.01447409 | \n",
       "| 09/07/2017  | 350.61      | 4239200     | 5           |  0.01764719 | \n",
       "| 09/08/2017  | 343.40      | 3263500     | 6           | -0.02056413 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Date       ClosePrice Volume  DayOfWeek Return     \n",
       "1 09/01/2017 355.40     3049500 6                  NA\n",
       "2 09/05/2017 349.59     3835100 3         -0.01634777\n",
       "3 09/06/2017 344.53     4091400 4         -0.01447409\n",
       "4 09/07/2017 350.61     4239200 5          0.01764719\n",
       "5 09/08/2017 343.40     3263500 6         -0.02056413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "-0.000228439843793543"
      ],
      "text/latex": [
       "-0.000228439843793543"
      ],
      "text/markdown": [
       "-0.000228439843793543"
      ],
      "text/plain": [
       "[1] -0.0002284398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0293739206974521"
      ],
      "text/latex": [
       "0.0293739206974521"
      ],
      "text/markdown": [
       "0.0293739206974521"
      ],
      "text/plain": [
       "[1] 0.02937392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tsla.df$DayOfWeek: 2\n",
       "[1] 0.0007221525\n",
       "------------------------------------------------------------ \n",
       "tsla.df$DayOfWeek: 3\n",
       "[1] 0.002809908\n",
       "------------------------------------------------------------ \n",
       "tsla.df$DayOfWeek: 4\n",
       "[1] 0.001490859\n",
       "------------------------------------------------------------ \n",
       "tsla.df$DayOfWeek: 5\n",
       "[1] -0.001767696\n",
       "------------------------------------------------------------ \n",
       "tsla.df$DayOfWeek: 6\n",
       "[1] -0.004363803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>3:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{3:} 2"
      ],
      "text/markdown": [
       "**3:** 2"
      ],
      "text/plain": [
       "3 \n",
       "2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>3:</strong> 0.00280990828919627"
      ],
      "text/latex": [
       "\\textbf{3:} 0.00280990828919627"
      ],
      "text/markdown": [
       "**3:** 0.00280990828919627"
      ],
      "text/plain": [
       "          3 \n",
       "0.002809908 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Q2 (20pts.)\n",
    "\n",
    "#a. What is a key difference between a matrix and a data frame?\n",
    "# Answer: \n",
    "# A matrix can only store columns of the same type whereas a data frame can \n",
    "# store column of different types.\n",
    "\n",
    "# You will have read in a data frame called  \"tsla.df\" which contains information on the price of Tesla stock\n",
    "\n",
    "#b How many rows and columns does this data frame have?\n",
    "# Answer: \n",
    "# 252 rows and 4 columns\n",
    "dim(tsla.df)\n",
    "\n",
    "#c On which day was Tesla's stock price maximized? Show your code that identifies the day. \n",
    "#  The function \"which.max\" can be very useful.\n",
    "# Answer:\n",
    "# 9/18/2017\n",
    "tsla.df$Date[which.max(tsla.df$ClosePrice)]\n",
    "\n",
    "#d Create and add to the data frame a new column called \"Return\" that calculates \n",
    "# the daily relative return of Tesla stock. The Daily Return is defined as \n",
    "# (price today - price yesterday)/price yesterday.\n",
    "# I suggest that you build this vector outside of the data frame, and when \n",
    "# complete, \"cbind\" it into the \n",
    "#  data frame. The first element in this \"return\" vector will have to be an NA \n",
    "# (without quotes) because there is no preceding day for it.\n",
    "#  Provide your code that builds the return vector and the code that adds it \n",
    "# back into the data frame.\n",
    "#  There are many ways to create this vector of returns, but you may want to \n",
    "# have a look at the \"diff\" function \n",
    "#  as an elegant way to perform the calculation. Use the \"head\" function to \n",
    "# print the first 5 rows of the data frame\n",
    "#  that now includs the Retun variable.\n",
    "# Output:\n",
    "# Date\tClosePrice\tVolume\tDayOfWeek\tReturn\n",
    "# 09/01/2017\t355.40\t3049500\t6\tNA\n",
    "# 09/05/2017\t349.59\t3835100\t3\t-0.01634777\n",
    "# 09/06/2017\t344.53\t4091400\t4\t-0.01447409\n",
    "# 09/07/2017\t350.61\t4239200\t5\t0.01764719\n",
    "# 09/08/2017\t343.40\t3263500\t6\t-0.02056413\n",
    "\n",
    "Return <- c(NA,\n",
    "            diff(x = tsla.df$ClosePrice, lag = 1)/head(tsla.df$ClosePrice,\n",
    "                                                       n = -1))\n",
    "tsla.df$Return <- Return\n",
    "head(tsla.df, n = 5)\n",
    "\n",
    "#e. What was Tesla's average daily return over this time period? To deal with \n",
    "# the NA, use the \"na.rm\" \n",
    "#   argument to the \"mean\" function.\n",
    "# Answer: -0.000228439843793543\n",
    "mean(tsla.df$Return, na.rm = TRUE)\n",
    "\n",
    "#f. What was the standard deviation of Tesla's daily return over this time period?\n",
    "# Answer: 0.0293739206974521\n",
    "sd(tsla.df$Return, na.rm = TRUE)\n",
    "\n",
    "#g. There is a column in the data frame called \"DayOfWeek\". It has been entered \n",
    "# numerically, 1 for Sunday etc. Turn \"DayOfWeek\" into an *unordered* factor and\n",
    "# overwrite the old DayOfWeek variable in the data frame.\n",
    "# Show your code that creates and inserts the factor day-of-week variable.\n",
    "tsla.df$DayOfWeek <- factor(x = tsla.df$DayOfWeek, ordered = FALSE)\n",
    "\n",
    "#h. The \"by\" command is useful for summarizing one variable over the levels of \n",
    "# a categorical/factor variable. Read about the \"by\" command using R's help \n",
    "# function.\n",
    "\n",
    "#   Using the \"by\" command, find which Day of the week had the highest average \n",
    "# daily return for Tesla stock.\n",
    "#   Because the mean  for Friday will be NA due to the first day's return being \n",
    "# NA, you will need to pass\n",
    "#   the \"na.rm = TRUE\" argument into the mean function when \"by\" applies it.\n",
    "#   In the function syntax:\n",
    "#   by(data, INDICES, FUN, ..., simplify = TRUE)\n",
    "#   the three dots \"...\" is a special slot where you can pass in arguments to \n",
    "# the function FUN. \n",
    "#   This will allow you to find the mean daily return for Friday too. \n",
    "#   You need to write code to find the best day-of-the-week and then extract \n",
    "# the mean return in the best day-of-the-week.   \n",
    "\n",
    "#   Code:\n",
    "by(data = tsla.df$Return, INDICES = tsla.df$DayOfWeek,\n",
    "   FUN = mean, na.rm = TRUE\n",
    "   )\n",
    "which.max(by(data = tsla.df$Return, INDICES = tsla.df$DayOfWeek,\n",
    "   FUN = mean, na.rm = TRUE\n",
    "   ))\n",
    "by(data = tsla.df$Return, INDICES = tsla.df$DayOfWeek,\n",
    "   FUN = mean, na.rm = TRUE\n",
    "   )[which.max(by(data = tsla.df$Return, INDICES = tsla.df$DayOfWeek,\n",
    "   FUN = mean, na.rm = TRUE\n",
    "   ))]\n",
    "\n",
    "#   Best day-of-the-week: 3, Tuesday\n",
    "#   Average return in best day-of-the-week: 0.002809908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"slope:\"\n",
      "[1] 8.863057e-10\n",
      "[1] \"intercept:\"\n",
      "[1] -0.006901303\n",
      "[1] \"rmse:\"\n",
      "[1] 0.0291919\n",
      "[1] \"r_squared:\"\n",
      "[1] 0.0163056\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Volume</th><td>  1         </td><td>0.003517230 </td><td>0.0035172297</td><td>4.0867086   </td><td>0.04430901  </td></tr>\n",
       "\t<tr><th scope=row>DayOfWeek</th><td>  4         </td><td>0.001330086 </td><td>0.0003325215</td><td>0.3863605   </td><td>0.81830964  </td></tr>\n",
       "\t<tr><th scope=row>Residuals</th><td>245         </td><td>0.210859489 </td><td>0.0008606510</td><td>       NA   </td><td>        NA  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\\\\n",
       "\\hline\n",
       "\tVolume &   1          & 0.003517230  & 0.0035172297 & 4.0867086    & 0.04430901  \\\\\n",
       "\tDayOfWeek &   4          & 0.001330086  & 0.0003325215 & 0.3863605    & 0.81830964  \\\\\n",
       "\tResiduals & 245          & 0.210859489  & 0.0008606510 &        NA    &         NA  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Df | Sum Sq | Mean Sq | F value | Pr(>F) | \n",
       "|---|---|---|\n",
       "| Volume |   1          | 0.003517230  | 0.0035172297 | 4.0867086    | 0.04430901   | \n",
       "| DayOfWeek |   4          | 0.001330086  | 0.0003325215 | 0.3863605    | 0.81830964   | \n",
       "| Residuals | 245          | 0.210859489  | 0.0008606510 |        NA    |         NA   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "          Df  Sum Sq      Mean Sq      F value   Pr(>F)    \n",
       "Volume      1 0.003517230 0.0035172297 4.0867086 0.04430901\n",
       "DayOfWeek   4 0.001330086 0.0003325215 0.3863605 0.81830964\n",
       "Residuals 245 0.210859489 0.0008606510        NA         NA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Volume</th><td>  1         </td><td>0.003517230 </td><td>0.0035172297</td><td>4.2017782   </td><td>0.04146403  </td></tr>\n",
       "\t<tr><th scope=row>DayOfWeek</th><td>  4         </td><td>0.001330086 </td><td>0.0003325215</td><td>0.3972392   </td><td>0.81053517  </td></tr>\n",
       "\t<tr><th scope=row>Volume:DayOfWeek</th><td>  4         </td><td>0.009122909 </td><td>0.0022807271</td><td>2.7246186   </td><td>0.03011001  </td></tr>\n",
       "\t<tr><th scope=row>Residuals</th><td>241         </td><td>0.201736580 </td><td>0.0008370812</td><td>       NA   </td><td>        NA  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\\\\n",
       "\\hline\n",
       "\tVolume &   1          & 0.003517230  & 0.0035172297 & 4.2017782    & 0.04146403  \\\\\n",
       "\tDayOfWeek &   4          & 0.001330086  & 0.0003325215 & 0.3972392    & 0.81053517  \\\\\n",
       "\tVolume:DayOfWeek &   4          & 0.009122909  & 0.0022807271 & 2.7246186    & 0.03011001  \\\\\n",
       "\tResiduals & 241          & 0.201736580  & 0.0008370812 &        NA    &         NA  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Df | Sum Sq | Mean Sq | F value | Pr(>F) | \n",
       "|---|---|---|---|\n",
       "| Volume |   1          | 0.003517230  | 0.0035172297 | 4.2017782    | 0.04146403   | \n",
       "| DayOfWeek |   4          | 0.001330086  | 0.0003325215 | 0.3972392    | 0.81053517   | \n",
       "| Volume:DayOfWeek |   4          | 0.009122909  | 0.0022807271 | 2.7246186    | 0.03011001   | \n",
       "| Residuals | 241          | 0.201736580  | 0.0008370812 |        NA    |         NA   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                 Df  Sum Sq      Mean Sq      F value   Pr(>F)    \n",
       "Volume             1 0.003517230 0.0035172297 4.2017782 0.04146403\n",
       "DayOfWeek          4 0.001330086 0.0003325215 0.3972392 0.81053517\n",
       "Volume:DayOfWeek   4 0.009122909 0.0022807271 2.7246186 0.03011001\n",
       "Residuals        241 0.201736580 0.0008370812        NA         NA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Q3 Regression (12pts.) \n",
    "\n",
    "#a Using the tsla.df data frame, run a regression of Tesla returns \n",
    "#  against daily volume and store the answer as a variable.\n",
    "lm.out <- lm(Return ~ Volume, data = tsla.df)\n",
    "\n",
    "#b Write code to extract the slope and intercept, RMSE and R^2 for the regression.\n",
    "# Answer:\n",
    "# slope: 8.863057e-10\n",
    "# intercept: -0.006901303\n",
    "# RMSE: 0.0291919\n",
    "# R^2: 0.0163056\n",
    "reg.summary <- summary(lm.out)\n",
    "reg.slope <- reg.summary$coefficients[2, 1]\n",
    "reg.intercept <- reg.summary$coefficients[1, 1]\n",
    "reg.rmse <- reg.summary[['sigma']]\n",
    "reg.r_squared <- reg.summary[['r.squared']]\n",
    "\n",
    "print(\"slope:\")\n",
    "print(reg.slope)\n",
    "\n",
    "print(\"intercept:\")\n",
    "print(reg.intercept)\n",
    "\n",
    "print(\"rmse:\")\n",
    "print(reg.rmse)\n",
    "\n",
    "print(\"r_squared:\")\n",
    "print(reg.r_squared)\n",
    "\n",
    "#c Now add in the categorical variable, DayOfWeek, to the regression. Is Day of \n",
    "# Week, in the presence of the\n",
    "#  daily volume, a statistically significant predictor of Tesla's return?   \n",
    "#  Show the code  you used to generate the relevant output as well as your conclusion.\n",
    "# Output:\n",
    "# \tDf\tSum Sq\tMean Sq\tF value\tPr(>F)\n",
    "# Volume\t1\t0.003517230\t0.0035172297\t4.0867086\t0.04430901\n",
    "# DayOfWeek\t4\t0.001330086\t0.0003325215\t0.3863605\t0.81830964\n",
    "# Residuals\t245\t0.210859489\t0.0008606510\tNA\tNA\n",
    "\n",
    "# Answer: Because the p-value of DayofWeek is so large, I conclude that\n",
    "# DayofWeek is not statistically significant.\n",
    "lm2.out <- lm(Return ~ Volume + DayOfWeek, data = tsla.df)\n",
    "anova(lm2.out)\n",
    "\n",
    "#d Finally, include in the regression the interaction between Volume and Day of \n",
    "# Week. \n",
    "#  Is this interaction  term statistically significant? Again, post your code \n",
    "# and state a conclusion.\n",
    "# Output:\n",
    "# \tDf\tSum Sq\tMean Sq\tF value\tPr(>F)\n",
    "# Volume\t1\t0.003517230\t0.0035172297\t4.2017782\t0.04146403\n",
    "# DayOfWeek\t4\t0.001330086\t0.0003325215\t0.3972392\t0.81053517\n",
    "# Volume:DayOfWeek\t4\t0.009122909\t0.0022807271\t2.7246186\t0.03011001\n",
    "# Residuals\t241\t0.201736580\t0.0008370812\tNA\tNA\n",
    "# Answer: Since the p-values of the interaction term have a p-value < 0.05, I\n",
    "# conclude that the interaction term is statistically significant.\n",
    "\n",
    "lm3.out <- lm(Return ~ Volume + DayOfWeek + Volume:DayOfWeek, data = tsla.df)\n",
    "anova(lm3.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>1590556744:</strong> 43"
      ],
      "text/latex": [
       "\\textbf{1590556744:} 43"
      ],
      "text/markdown": [
       "**1590556744:** 43"
      ],
      "text/plain": [
       "1590556744 \n",
       "        43 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>$`1590556744`</strong> = 77"
      ],
      "text/latex": [
       "\\textbf{\\$`1590556744`} = 77"
      ],
      "text/markdown": [
       "**$`1590556744`** = 77"
      ],
      "text/plain": [
       "$`1590556744`\n",
       "[1] 77\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### The lapply function\n",
    "#### Q4. (12pts.) you will have read in a list called \"gala.list\". It contains \n",
    "#### the text based responses \n",
    "#### to a survey question that asked individuals what they liked \"least\" about \n",
    "#### a gala event.\n",
    "#### The names in the list are respondent IDs (coerced to characters) and the \n",
    "#### associated element is the text response.\n",
    "\n",
    "#a. Using repeated applications of the \"lapply\" command and \"unlist\" command, \n",
    "# write R code to find\n",
    "#   which respondent wrote the most words in their answer. Recall that \n",
    "# \"strsplit\" will split a character string\n",
    "#   and the \"length\" command will count how many elements there are in a vector.\n",
    "# The command \"which.max\" will identify\n",
    "#   the maximum element in a vector. Hint: you need to realize that \"strsplit\" \n",
    "# itself returns a list structure.\n",
    "#   To get things started, lapply(gala.list,strsplit,\" \") breaks each \n",
    "# respondent's answer into distinct words\n",
    "#   There are alternative approaches to doing this question, but I want you to \n",
    "# use lapply.\n",
    "# Answer:\n",
    "# Individual 1590556744 wrote the most words with 77.\n",
    "\n",
    "splitted_string <- lapply(X = gala.list, FUN = strsplit, \" \")\n",
    "unlisted_strings <- lapply(X = splitted_string, FUN = unlist)\n",
    "word_count_by_individual <- lapply(X = unlisted_strings, FUN = length)\n",
    "which.max(x = word_count_by_individual)\n",
    "word_count_by_individual[which.max(x = word_count_by_individual)]\n",
    "\n",
    "#b. Briefly explain each what each step in the code you wrote for part (a) does.  \n",
    "# In particular, describe the \n",
    "#   data structure (vector, matrix, list, etc.) returned by each function you call.\n",
    "# Answer: \n",
    "# First, I used lapply to split the comments from each individual. This\n",
    "# resulted in a list of list, with each element in the first level of list\n",
    "# representing an individual and each individual's list of words from their\n",
    "# comment. Next, I used lapply to unlist each individual's comment word list.\n",
    "# This returned, a list of vectors. Each element represented an individual and\n",
    "# each element had a vector inside it that represented their comment words.\n",
    "# Then I used lapply and applied the lenght function. This returned a list of\n",
    "# integers. Each element of the list represented an individual and each of them\n",
    "# contained an integer that represented the lenght of the vector from the last\n",
    "# step.\n",
    "# Finally I used which.max to find which element of the list had the largest\n",
    "# lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
